{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "from collections import Counter\n",
    "\n",
    "import utils.load_data as load_data\n",
    "from models.emotion_recognition import ClassifyEmotion\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished data session Session1\n",
      "Finished data session Session2\n",
      "Finished data session Session3\n",
      "Finished data session Session4\n",
      "Finished data session Session5\n"
     ]
    }
   ],
   "source": [
    "data, orig_lengths, labels, metadata = load_data.load_sentences(max_sequence_length=750, \n",
    "                                                                win_len=.04, win_step=.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Y to one hot\n",
    "int_labels = LabelEncoder().fit_transform(labels)\n",
    "int_labels = int_labels.reshape(len(int_labels), 1)\n",
    "onehot_labels = OneHotEncoder().fit_transform(int_labels).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset limited amount of data for hyperparameter search \n",
    "lim_data = data[::5]\n",
    "lim_lengths = orig_lengths[::5] \n",
    "lim_onehot_labels = onehot_labels[::5]\n",
    "\n",
    "# Split into train and test\n",
    "lim_train_x, lim_val_x, lim_train_y, lim_val_y, lim_train_lengths, lim_val_lengths = train_test_split(lim_data,\n",
    "                                                                              lim_onehot_labels,\n",
    "                                                                              lim_lengths,\n",
    "                                                                              test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search using limited data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_features': 26, 'num_classes': 6, 'learning_rate': 0.0001, 'num_hidden': 256}\n",
      "Created 42 batches of size 32\n",
      "Created Validation 5 batches of size 32\n",
      "Data Batch Shape: (32, 750, 26) Labels Shape: (32, 6) \n",
      "(?, 512)\n",
      "(?, 512)\n",
      "(?, 128)\n",
      "(?, 6)\n",
      "Epoch 0 mean batch val loss 1.7027305364608765\n",
      "Epoch 5 mean batch val loss 1.6789894104003906\n",
      "Epoch 10 mean batch val loss 1.6803194284439087\n",
      "Epoch 15 mean batch val loss 1.6795856952667236\n",
      "Epoch 20 mean batch val loss 1.679100751876831\n",
      "Epoch 25 mean batch val loss 1.678067922592163\n",
      "Epoch 30 mean batch val loss 1.6766560077667236\n",
      "Epoch 35 mean batch val loss 1.6761207580566406\n",
      "Epoch 40 mean batch val loss 1.6761928796768188\n",
      "Epoch 45 mean batch val loss 1.6723095178604126\n",
      "Accuracy: 0.19594594594594594\n",
      "{'num_features': 26, 'num_classes': 6, 'learning_rate': 0.0005, 'num_hidden': 256}\n",
      "Created 42 batches of size 32\n",
      "Created Validation 5 batches of size 32\n",
      "Data Batch Shape: (32, 750, 26) Labels Shape: (32, 6) \n",
      "(?, 512)\n",
      "(?, 512)\n",
      "(?, 128)\n",
      "(?, 6)\n",
      "Epoch 0 mean batch val loss 1.675321340560913\n",
      "Epoch 5 mean batch val loss 1.6799644231796265\n",
      "Epoch 10 mean batch val loss 1.676016092300415\n",
      "Epoch 15 mean batch val loss 1.6754230260849\n",
      "Epoch 20 mean batch val loss 1.6740119457244873\n",
      "Epoch 25 mean batch val loss 1.679764986038208\n",
      "Epoch 30 mean batch val loss 1.674290418624878\n",
      "Epoch 35 mean batch val loss 1.6725143194198608\n",
      "Epoch 40 mean batch val loss 1.6681602001190186\n",
      "Epoch 45 mean batch val loss 1.668857216835022\n",
      "Accuracy: 0.25675675675675674\n",
      "{'num_features': 26, 'num_classes': 6, 'learning_rate': 0.0007, 'num_hidden': 256}\n",
      "Created 42 batches of size 32\n",
      "Created Validation 5 batches of size 32\n",
      "Data Batch Shape: (32, 750, 26) Labels Shape: (32, 6) \n",
      "(?, 512)\n",
      "(?, 512)\n",
      "(?, 128)\n",
      "(?, 6)\n",
      "Epoch 0 mean batch val loss 1.6847164630889893\n",
      "Epoch 5 mean batch val loss 1.6778185367584229\n",
      "Epoch 10 mean batch val loss 1.672260046005249\n",
      "Epoch 15 mean batch val loss 1.6722509860992432\n",
      "Epoch 20 mean batch val loss 1.6722805500030518\n",
      "Epoch 25 mean batch val loss 1.6709082126617432\n",
      "Epoch 30 mean batch val loss 1.6798499822616577\n",
      "Epoch 35 mean batch val loss 1.679142951965332\n",
      "Epoch 40 mean batch val loss 1.697752594947815\n",
      "Epoch 45 mean batch val loss 1.6820316314697266\n",
      "Accuracy: 0.1891891891891892\n",
      "{'num_features': 26, 'num_classes': 6, 'learning_rate': 0.001, 'num_hidden': 256}\n",
      "Created 42 batches of size 32\n",
      "Created Validation 5 batches of size 32\n",
      "Data Batch Shape: (32, 750, 26) Labels Shape: (32, 6) \n",
      "(?, 512)\n",
      "(?, 512)\n",
      "(?, 128)\n",
      "(?, 6)\n",
      "Epoch 0 mean batch val loss 1.6796996593475342\n",
      "Epoch 5 mean batch val loss 1.6773306131362915\n",
      "Epoch 10 mean batch val loss 1.6705747842788696\n",
      "Epoch 15 mean batch val loss 1.6667448282241821\n",
      "Epoch 20 mean batch val loss 1.6662118434906006\n",
      "Epoch 25 mean batch val loss 1.6749944686889648\n",
      "Epoch 30 mean batch val loss 1.6728004217147827\n",
      "Epoch 35 mean batch val loss 1.6610063314437866\n",
      "Epoch 40 mean batch val loss 1.6725871562957764\n",
      "Epoch 45 mean batch val loss 1.742483139038086\n",
      "Accuracy: 0.24324324324324326\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [.0001, .0005, .0007, .001]\n",
    "truth = np.argmax(lim_val_y, 1) \n",
    "\n",
    "for lr in learning_rates:\n",
    "    lstm = ClassifyEmotion(lr=lr, num_hidden=256)\n",
    "    print(vars(lstm))\n",
    "    val_pred = lstm.train(train_x=lim_train_x, val_x=lim_val_x, \n",
    "                          train_y=lim_train_y, val_y=lim_val_y, \n",
    "                          train_lengths=lim_train_lengths, \n",
    "                          val_lengths=lim_val_lengths, \n",
    "                          verbose=False, \n",
    "                          batch_size=32, epochs=50, load_model=False, \n",
    "                          save_path='final_model/lr_test.ckpt')\n",
    "    acc = sum(val_pred == np.argmax(lim_val_y, 1))/len(val_pred)\n",
    "    print('Accuracy: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_features': 26, 'num_classes': 6, 'learning_rate': 0.0005, 'num_hidden': 512}\n",
      "Created 42 batches of size 32\n",
      "Created Validation 5 batches of size 32\n",
      "Data Batch Shape: (32, 750, 26) Labels Shape: (32, 6) \n",
      "(?, 1024)\n",
      "(?, 1024)\n",
      "(?, 256)\n",
      "(?, 6)\n",
      "Epoch 0 mean batch val loss 1.6801049709320068\n",
      "Epoch 5 mean batch val loss 1.6791181564331055\n",
      "Epoch 10 mean batch val loss 1.6758193969726562\n",
      "Epoch 15 mean batch val loss 1.6723060607910156\n",
      "Epoch 20 mean batch val loss 1.7721226215362549\n",
      "Epoch 25 mean batch val loss 1.736720323562622\n",
      "Epoch 30 mean batch val loss 1.733214020729065\n",
      "Epoch 35 mean batch val loss 1.728166937828064\n",
      "Epoch 40 mean batch val loss 1.7222931385040283\n",
      "Epoch 45 mean batch val loss 1.715497612953186\n",
      "Accuracy: 0.25\n",
      "{'num_features': 26, 'num_classes': 6, 'learning_rate': 0.0005, 'num_hidden': 128}\n",
      "Created 42 batches of size 32\n",
      "Created Validation 5 batches of size 32\n",
      "Data Batch Shape: (32, 750, 26) Labels Shape: (32, 6) \n",
      "(?, 256)\n",
      "(?, 256)\n",
      "(?, 64)\n",
      "(?, 6)\n",
      "Epoch 0 mean batch val loss 1.6830295324325562\n",
      "Epoch 5 mean batch val loss 1.6776962280273438\n",
      "Epoch 10 mean batch val loss 1.671523094177246\n",
      "Epoch 15 mean batch val loss 1.6703250408172607\n",
      "Epoch 20 mean batch val loss 1.6647762060165405\n",
      "Epoch 25 mean batch val loss 1.6592369079589844\n",
      "Epoch 30 mean batch val loss 1.6718149185180664\n",
      "Epoch 35 mean batch val loss 1.6593239307403564\n",
      "Epoch 40 mean batch val loss 1.6648643016815186\n",
      "Epoch 45 mean batch val loss 1.664939522743225\n",
      "Accuracy: 0.20270270270270271\n",
      "{'num_features': 26, 'num_classes': 6, 'learning_rate': 0.0005, 'num_hidden': 64}\n",
      "Created 42 batches of size 32\n",
      "Created Validation 5 batches of size 32\n",
      "Data Batch Shape: (32, 750, 26) Labels Shape: (32, 6) \n",
      "(?, 128)\n",
      "(?, 128)\n",
      "(?, 32)\n",
      "(?, 6)\n",
      "Epoch 0 mean batch val loss 1.700989007949829\n",
      "Epoch 5 mean batch val loss 1.6753206253051758\n",
      "Epoch 10 mean batch val loss 1.6685558557510376\n",
      "Epoch 15 mean batch val loss 1.6681592464447021\n",
      "Epoch 20 mean batch val loss 1.6567184925079346\n",
      "Epoch 25 mean batch val loss 1.651473045349121\n",
      "Epoch 30 mean batch val loss 1.6535756587982178\n",
      "Epoch 35 mean batch val loss 1.663648247718811\n",
      "Epoch 40 mean batch val loss 1.6639769077301025\n",
      "Epoch 45 mean batch val loss 1.6430658102035522\n",
      "Accuracy: 0.19594594594594594\n"
     ]
    }
   ],
   "source": [
    "num_hidden = [512, 128, 64]\n",
    "\n",
    "for h in num_hidden: \n",
    "    lstm = ClassifyEmotion(lr=.0005, num_hidden=h)\n",
    "    print(vars(lstm))\n",
    "    val_pred = lstm.train(train_x=lim_train_x, val_x=lim_val_x, \n",
    "                          train_y=lim_train_y, val_y=lim_val_y, \n",
    "                          train_lengths=lim_train_lengths, \n",
    "                          val_lengths=lim_val_lengths, \n",
    "                          verbose=False, \n",
    "                          batch_size=32, epochs=50, load_model=False, \n",
    "                          save_path='final_model/num_hidden_test.ckpt')\n",
    "    acc = sum(val_pred == np.argmax(lim_val_y, 1))/len(val_pred)\n",
    "    print('Accuracy: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_features': 26, 'num_classes': 6, 'learning_rate': 0.0005, 'num_hidden': 64, 'dense_hidden': 512}\n",
      "Created 42 batches of size 32\n",
      "Created Validation 5 batches of size 32\n",
      "Data Batch Shape: (32, 750, 26) Labels Shape: (32, 6) \n",
      "(?, 128)\n",
      "(?, 128)\n",
      "(?, 512)\n",
      "(?, 6)\n",
      "Epoch 0 mean batch val loss 1.6846727132797241\n",
      "Epoch 5 mean batch val loss 1.6806259155273438\n",
      "Epoch 10 mean batch val loss 1.6762855052947998\n",
      "Epoch 15 mean batch val loss 1.6878869533538818\n",
      "Epoch 20 mean batch val loss 1.6715717315673828\n",
      "Epoch 25 mean batch val loss 1.666272521018982\n",
      "Epoch 30 mean batch val loss 1.6656242609024048\n",
      "Epoch 35 mean batch val loss 1.6539623737335205\n",
      "Epoch 40 mean batch val loss 1.6508935689926147\n",
      "Epoch 45 mean batch val loss 1.6635900735855103\n",
      "Epoch 49 mean batch val loss 1.6635900735855103\n"
     ]
    }
   ],
   "source": [
    "# Dense layer hidden units = 512 \n",
    "lstm = ClassifyEmotion(lr=.0005, num_hidden=64, dense_hidden=512)\n",
    "print(vars(lstm))\n",
    "val_pred = lstm.train(train_x=lim_train_x, val_x=lim_val_x, \n",
    "                      train_y=lim_train_y, val_y=lim_val_y, \n",
    "                      train_lengths=lim_train_lengths, \n",
    "                      val_lengths=lim_val_lengths, \n",
    "                      verbose=False, \n",
    "                      batch_size=32, epochs=50, load_model=False, \n",
    "                      save_path='final_model/num_hidden_test.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.20270270270270271\n"
     ]
    }
   ],
   "source": [
    "acc = sum(val_pred == np.argmax(lim_val_y, 1))/len(val_pred)\n",
    "print('Accuracy: {}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train using all data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "train_x, val_x, train_y, val_y, train_lengths, val_lengths = train_test_split(data,\n",
    "                                                                              onehot_labels,\n",
    "                                                                              orig_lengths,\n",
    "                                                                              test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_hidden': 128, 'num_classes': 6, 'num_features': 26, 'learning_rate': 0.0005, 'dense_hidden': 512}\n",
      "Created 208 batches of size 32\n",
      "Created Validation 24 batches of size 32\n",
      "Data Batch Shape: (32, 750, 26) Labels Shape: (32, 6) \n",
      "(?, 256)\n",
      "(?, 256)\n",
      "(?, 512)\n",
      "(?, 6)\n",
      "Epoch 0 mean loss 1.7397600412368774\n",
      "Epoch 0 mean batch val loss 2.1266140937805176\n",
      "Epoch 1 mean loss 1.7312729358673096\n",
      "Epoch 2 mean loss 1.7293425798416138\n",
      "Epoch 3 mean loss 1.7284976243972778\n",
      "Epoch 4 mean loss 1.7279680967330933\n"
     ]
    }
   ],
   "source": [
    "lstm = ClassifyEmotion(lr=.0005, num_hidden=128, dense_hidden=512)\n",
    "print(vars(lstm))\n",
    "val_predictions = lstm.train(train_x=train_x, val_x=val_x, train_y=train_y, val_y=val_y, \n",
    "                             train_lengths=train_lengths, val_lengths=val_lengths, \n",
    "                             batch_size=32, epochs=20, load_model=False, verbose=True,\n",
    "                             save_path='final_model/emotion_recognition_750.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
